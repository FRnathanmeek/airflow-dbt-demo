{"metadata": {"dbt_schema_version": "https://schemas.getdbt.com/dbt/run-results/v4.json", "dbt_version": "1.5.6", "generated_at": "2023-08-23T21:19:51.339587Z", "invocation_id": "356154f6-34c5-4550-b8b5-6217dee8e71b", "env": {}}, "results": [{"status": "error", "timing": [], "thread_id": "Thread-1", "execution_time": 0.9227945804595947, "adapter_response": {}, "message": "Runtime Error in model issues_staged (models/sources/diplo/staged/issues_staged.sql)\n  Database Error\n    org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: stg_raw_data.issues; line 19 pos 9;\n    'CreateViewCommand `issues_staged__dbt_tmp`, with source_cte as (\n        select\n        _cdc_timestamp,\n    \t_cdc_id,\n    \t_org_name,\n    \t_cdc_lsn,\n    \t_cdc_processed_timestamp,\n    \t_cdc_transaction_id,\n        \n        ROW_NUMBER() OVER(\n                PARTITION BY id, _org_name\n                ORDER BY _cdc_timestamp desc, _cdc_lsn desc, _cdc_processed_timestamp desc\n            ) as row_number,\n        *\n        from stg_raw_data.issues\n        where _cdc_timestamp > (select nvl(max(_cdc_timestamp),\"1970-01-02 00:00:00 UTC\") from staging_pub_analytics_staged.`issues_staged`)\n    )\n    select\n        \n    \n        CONCAT(id, '_' , _org_name) as ion_uid,\n        -- TEMP addition for snowflake because _org_name is partitioned upon it isnt in the parquet files\n        _org_name as _organization_name\n    \n    ,\n        *\n    from source_cte\n    where \n        row_number = 1, false, false, LocalTempView, false\n    +- 'Project ['CONCAT('id, _, '_org_name) AS ion_uid#3652864, '_org_name AS _organization_name#3652865, *]\n       +- 'Filter ('row_number = 1)\n          +- 'SubqueryAlias source_cte\n             +- 'Project ['_cdc_timestamp, '_cdc_id, '_org_name, '_cdc_lsn, '_cdc_processed_timestamp, '_cdc_transaction_id, row_number() windowspecdefinition('id, '_org_name, '_cdc_timestamp DESC NULLS LAST, '_cdc_lsn DESC NULLS LAST, '_cdc_processed_timestamp DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number#3652866, *]\n                +- 'Filter ('_cdc_timestamp > scalar-subquery#3652867 [])\n                   :  +- 'Project [unresolvedalias('nvl('max('_cdc_timestamp), 1970-01-02 00:00:00 UTC), None)]\n                   :     +- 'UnresolvedRelation [staging_pub_analytics_staged, issues_staged], [], false\n                   +- 'UnresolvedRelation [stg_raw_data, issues], [], false\n    \n    \tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n    \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:325)\n    \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:230)\n    \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n    \tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n    \tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n    \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\n    \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:230)\n    \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:225)\n    \tat java.base/java.security.AccessController.doPrivileged(Native Method)\n    \tat java.base/javax.security.auth.Subject.doAs(Unknown Source)\n    \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n    \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:239)\n    \tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n    \tat java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n    \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n    \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n    \tat java.base/java.lang.Thread.run(Unknown Source)\n    Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: stg_raw_data.issues; line 19 pos 9;\n    'CreateViewCommand `issues_staged__dbt_tmp`, with source_cte as (\n        select\n        _cdc_timestamp,\n    \t_cdc_id,\n    \t_org_name,\n    \t_cdc_lsn,\n    \t_cdc_processed_timestamp,\n    \t_cdc_transaction_id,\n        \n        ROW_NUMBER() OVER(\n                PARTITION BY id, _org_name\n                ORDER BY _cdc_timestamp desc, _cdc_lsn desc, _cdc_processed_timestamp desc\n            ) as row_number,\n        *\n        from stg_raw_data.issues\n        where _cdc_timestamp > (select nvl(max(_cdc_timestamp),\"1970-01-02 00:00:00 UTC\") from staging_pub_analytics_staged.`issues_staged`)\n    )\n    select\n        \n    \n        CONCAT(id, '_' , _org_name) as ion_uid,\n        -- TEMP addition for snowflake because _org_name is partitioned upon it isnt in the parquet files\n        _org_name as _organization_name\n    \n    ,\n        *\n    from source_cte\n    where \n        row_number = 1, false, false, LocalTempView, false\n    +- 'Project ['CONCAT('id, _, '_org_name) AS ion_uid#3652864, '_org_name AS _organization_name#3652865, *]\n       +- 'Filter ('row_number = 1)\n          +- 'SubqueryAlias source_cte\n             +- 'Project ['_cdc_timestamp, '_cdc_id, '_org_name, '_cdc_lsn, '_cdc_processed_timestamp, '_cdc_transaction_id, row_number() windowspecdefinition('id, '_org_name, '_cdc_timestamp DESC NULLS LAST, '_cdc_lsn DESC NULLS LAST, '_cdc_processed_timestamp DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number#3652866, *]\n                +- 'Filter ('_cdc_timestamp > scalar-subquery#3652867 [])\n                   :  +- 'Project [unresolvedalias('nvl('max('_cdc_timestamp), 1970-01-02 00:00:00 UTC), None)]\n                   :     +- 'UnresolvedRelation [staging_pub_analytics_staged, issues_staged], [], false\n                   +- 'UnresolvedRelation [stg_raw_data, issues], [], false\n    \n    \tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n    \tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:126)\n    \tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:97)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:263)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:262)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:262)\n    \tat scala.collection.Iterator.foreach(Iterator.scala:943)\n    \tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n    \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n    \tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n    \tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n    \tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:262)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:262)\n    \tat scala.collection.Iterator.foreach(Iterator.scala:943)\n    \tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n    \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n    \tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n    \tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n    \tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:262)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:262)\n    \tat scala.collection.Iterator.foreach(Iterator.scala:943)\n    \tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n    \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n    \tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n    \tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n    \tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:262)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:262)\n    \tat scala.collection.Iterator.foreach(Iterator.scala:943)\n    \tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n    \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n    \tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n    \tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n    \tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:262)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:262)\n    \tat scala.collection.Iterator.foreach(Iterator.scala:943)\n    \tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n    \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n    \tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n    \tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n    \tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:262)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:262)\n    \tat scala.collection.immutable.List.foreach(List.scala:431)\n    \tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)\n    \tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:97)\n    \tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:92)\n    \tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:182)\n    \tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:205)\n    \tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n    \tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)\n    \tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)\n    \tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n    \tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)\n    \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n    \tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)\n    \tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)\n    \tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)\n    \tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:65)\n    \tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n    \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n    \tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n    \tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n    \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n    \tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n    \tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n    \tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:291)\n    \t... 16 more\n    ", "failures": null, "unique_id": "model.hush_sound.issues_staged"}], "elapsed_time": 22.635812520980835, "args": {"exclude": [], "favor_state": false, "defer": false, "log_level": "info", "populate_cache": true, "indirect_selection": "eager", "enable_legacy_logger": false, "profiles_dir": ".", "cache_selected_only": false, "select": ["issues_staged"], "static_parser": true, "send_anonymous_usage_stats": false, "quiet": false, "warn_error_options": {"include": [], "exclude": []}, "which": "run", "log_path": "/opt/airflow/dbt/logs", "partial_parse": true, "log_level_file": "debug", "log_format": "default", "vars": {}, "strict_mode": false, "project_dir": "/opt/airflow/dbt", "log_file_max_bytes": 10485760, "version_check": true, "macro_debugging": false, "printer_width": 120, "use_colors": true, "use_colors_file": true, "print": true, "write_json": true, "introspect": true, "log_format_file": "debug"}}